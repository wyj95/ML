# 统计学习方法笔记

[toc]

/*

* 在笔记中$X = {x_1, x_2, ...... x_N}$表示数据集，其中$x_i = {(x_i^1, x_i^2, ...... x_i^n)}^T$表示一个数据向量

​	*/

## 感知机

### 对偶形式

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221104203941.png)



## k近邻法

### 介绍

*选择离目标点最近的k个点，然后投票决定目标点的标签*



### kd树的构建

*从$x^1$开始循环，以中位线对数据进行划分，中位线数据保留在原结点，其余结点分配到左右孩子*

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221104204310.png)

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221104204323.png)



### kd树的搜索

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221104204346.png)

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221104204410.png)



## 朴素贝叶斯法

### 条件

输入变量是独立变量



### 介绍

*计算出联合分布P(Y|X)，取最大概率作为测试数据的标签*



### 极大似然估计

*以频率代替概率*

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221104211422.png)



## 决策树

### 随机变量的熵

*衡量随便变量的分散程度，越分散越大，极限的话某个p是1则取得最小的熵 0*， 定义为
$$
P(X=x_i)=p_i
$$

$$
H(X)=-\sum p_ilog(p_i)
$$


### 信息增益

*获取到条件A后，数据集D的熵的减少量*，定义为：
$$
g(D, A)=H(D)-H(D|A)
$$


### 信息增益比

*平衡选择特征的数量*，定义为：
$$
g_R(D, A) = \frac{g(D, A)}{H_A(D)}
$$

$$
where ,H_A(D)=-\sum\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}
$$



### ID3

*计算所有特征的信息增益，取最大且超过阈值者，依其全部取值构造对应孩子，对各结点继续上诉算法*

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221104215757.png)

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221104215715.png)



### C4.5

*上面ID3算法指标取为信息增益比即可*



### 剪枝

*Loss 由模型准确率 即叶节点的熵$C(T)$和正则项 $|T|$组成*。公式如下：
$$
C_{\alpha}(T)=C(T)+\alpha|T|=\sum\limits^{|T|}_{t=1} N_tH_t(T)+\alpha|T|
$$

$$
H_t(T)=\sum\limits_k^K \frac {N_{tk}}{N_t}log\frac {N_{tk}}{N_t}
$$

其中

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221105174012.png)

*算法：先计算各个结点的熵，然后根据 Loss选择是否剪枝，讲孩子结点合并到父亲结点中*



### CART回归树

*采用最小二乘法，对所有的指标，选择最佳的划分，使得各部分各点到均值的距离二范数最小，不停进行*

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221105182033.png)



### CART分类树

*类似上诉算法，采用基尼指数作为评价指标*，定义如下：
$$
Gine(p)=\sum\limits^{K}_{k=1}p_k(1-p_k)=1-\sum\limits^K_{k=1}p_k^2,where, P\{x=k\}=p_k
$$

$$
Gine(D)=1-\sum\limits_{k=1}^K(\frac{|C_k|}{|D|})^2,where ,D\ is\  a\  dataset
$$



### CKRT 剪枝

*根据$\alpha$在逐渐变大的过程中，决策会从不剪枝变成剪枝，计算各个结点的该阈值，取最小的依次进行剪枝N次，采用投票表决类别，对生成的N棵树进行独立交叉验证，选择最佳的树（二乘或基尼）作为最终的树）*，算法：

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221105192911.png)



## 逻辑回归

*没什么好说的，Loss采用似然估计，梯度下降求解*

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221106131903.png)

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221106132100.png)

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221106131925.png)



## 最大熵模型



* 对于给定的数据集$T=\{(x_1, y_1), ...,(x_N, y_N)\}$，估计经验分布$\hat P(X, Y)\ 和\ \hat P(X))$，$f_i(x, y)$为特征函数，模型输出P(Y|X)

$$
P(X=x, Y=y)=\frac {v(X=x, Y=y)}{N},\ 
P(X=x)=\frac {v(X=x)}{N}
$$

* 约束条件为特征函数的期望经验值与模型值相等

$$
E_{\hat P}(f_i)=\sum \limits_{x, y} \hat P(x, y)f_i(x, y)=E_P(f_i)=\sum \limits_{x, y}\hat P(x)P(y|x)f(x, y)
$$

* 目标函数为概率分布熵值最大

$$
\mathop{max}_{P}\  H(P)=-\sum\limits_{x, y}\hat P(x)P(y|x)logP(y|x)
$$

$$
s.t. \ \ E_{\hat P}(f_i)=E_P(f_i)
$$

$$
\sum\limits_yP(y|x)=1
$$

* 拉格朗日原始约束

$$
L(P,w)=-H(P)+w_0(\sum\limits_yP(y|x)-1)+\sum\limits_{i=1}^n w_i(E_{\hat P}(f_i)-E_P(f_i))
$$

$$
\mathop{min}_P\ \mathop{max}_w\ L(P, w)
$$

* 对偶形式

$$
\mathop{max}_w\ \mathop{min}_P\ L(P,w)
$$

## 最优化方法

​	**有空细看**



## 支持向量机

### 线性可分支持向量机

*引入函数间隔* $\hat \gamma$*，后因为无法解决 w的线性增长带来的影响，故使用几何间隔* $\gamma$，定义如下：
$$
\hat\gamma_i=y_i(w*x_i+b),\ \ \  \gamma_i=\frac{\hat\gamma}{||w||}=\frac{y_i}{||w||}(w*x_i+b)
$$
对于数据集$T=\{x_1, ..., x_N\}$的函数（几何）间隔为：
$$
\hat\gamma=\mathop{min}_{i=1,...N}\hat\gamma_i,\ \ \gamma=\mathop{min}_{i=1,...,N}\gamma_i
$$
模型
$$
\mathop{max}_{w, b}\gamma
$$

$$
s.t.\ \ y_i(w*x_i+b)\geq\gamma
$$

等价于：
$$
\mathop{max}_{w, b}\frac{\hat\gamma}{||w||}=\mathop{min}_{w, b}\frac 1 2 ||w||^2
$$

$$
s.t.\ \ \frac{y_i}{||w||}(w*x_i+b)\geq\hat\gamma,\ \ 可取\hat\gamma=1
$$

对偶形式
$$
f(x)=sign(\sum\limits_{i=1}^N\alpha_i^*y_i(x*x_i)+b^*)
$$
模型：
$$
\mathop {min}_\alpha \frac 1 2 \sum\limits_{i=1}^N\sum\limits_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i*x_j)-\sum\limits_{i=1}^N\alpha_i
$$

$$
s.t.\ \ \sum\limits_{i=1}^N\alpha_iy_i=0,\ a_i \geq 0
$$

$$
and\ \ w^*=\sum\limits_{i=1}^N\alpha_i^*y_ix_i,
$$

$$
if\ \alpha_j^*>0,\ b^*=y_j-\sum\limits_{i=1}^N\alpha_i^*y_i(x_i*x_j)
$$

$$
then,\ f(x)=sign(w^**x+b^*)
$$

其中满足$\alpha_i>0$对应的点即为支持向量

### 软约束

$$
max \ \frac1 2 ||w||^2+C\sum\limits_{i=1}^N\xi_i
$$


$$
y_i(w*x_i+b)\geq1-\xi_i
$$

![image-20230101220603760](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20230101220603760.png)

### 核函数

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221108204419.png)

### 常见核函数

多项式核函数
$$
K(x,z)=(x*z+1)^p
$$
高斯核函数
$$
K(x, z)=exp(-\frac{||x-z||^2}{2\sigma^2})
$$
字符串核函数
$$
[\varphi_n(s)]_u=\sum\limits_{i:s(i)=u}\lambda^{l(i)},\ where \ 0\leq \lambda \leq 1
$$
其中$\varphi_n(s)$ 有u维，u为所有字符串的数量，i为序列，s(i)为s的子串，l(i)为i的长度，例子：

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221110164429.png)
$$
K_n(s,t)=\sum\limits_{u\in \sum^n}[\varphi_n(s)]_u[\varphi_n(t)]_u
$$


### 非线性支持向量机

*把对偶形式的x，y之间的度量用核函数取代*,如下：
$$
f(x)=sign(\sum\limits_{i=1}^N\alpha_i^*y_iK(x,x_i)+b^*)
$$
具体参考上面



### ！！！最优最小序列学习法（SMO）！！！

## 集成方法

### AdaBoost

*在分类器的权衡中：准确率高的分类器具有更高的权重；*

*在数据集的权重中：选择这一次错误的例子，下一个分类器要加大该训练集的权重；*

N个数据，M个分类器

1. 初始化权重$D=\{\frac 1 N.., \}*N$

2. 对一个分类器计算准确率和分类器权重$\alpha$
3. 更新数据集权重
4. 返回2继续
5. 最终分类器

$$
G(x)=sign(\ \sum\limits_{i=1}^M\alpha_iG_i(x)\ )
$$

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221111004815.png)

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221111004831.png)

### 前向分步算法

*逐步极小化* $\beta$ *和* $\gamma$，模型：
$$
f(x)=\sum\limits_{m=1}^1\beta_mb(x;\gamma_m)
$$
算法：

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221113161729.png)

若损失函数是指数损失函数，则该模型即AdaBoost
$$
L(y,f(x))=exp[-yf(x)]
$$

### 集成树

*相当于是在不断拟合残差*

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221113174035.png)

## EM

### 算法

[EM（最大期望）算法推导、GMM的应用与代码实现 - 颀周 - 博客园 (cnblogs.com)](https://www.cnblogs.com/qizhou/p/13100817.html)

![](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/20221124174713.png)

### Q function

$$
Q(\theta,\theta^{(i)})=E_Z[log\ P((Y, Z), \theta)|Y, \theta^{(i)}]
$$

$$
=\sum\limits_ZP(Z, \theta^{(i)})log\ P((Y, Z), \theta)
$$

Z的分布由$\theta^{(i)}$决定，(Y, Z)为具体的实例，Q最大可参考似然函数最大：
$$
f(Y)=\sum\limits_y\sum\limits_ZP(y, Z)=\sum\limits_Z P(Y, Z)
$$

### 高斯混合模型

![image-20221124181848634](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20221124181848634.png)

![image-20221124181902127](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20221124181902127.png)

具体见下：

![image-20221124183012502](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20221124183012502.png)

![image-20221124183043006](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20221124183043006.png)

![image-20221124183101380](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20221124183101380.png)

### GEM

![image-20221124183239711](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20221124183239711.png)

![image-20221124183302093](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20221124183302093.png)

![image-20221124183316704](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20221124183316704.png)

![image-20221124183325150](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20221124183325150.png)

## 隐马尔科夫模型

### 定义

N个状态，M个观测
$$
A^T_{ij}[N*N]表示第T阶段从状态i转移到状态j的概率
$$

$$
B^T_{ij}[N*M]表示在第T阶段状态i观察到观测j的概率
$$

$$
\pi_i[N]表示初始的状态分布即状态i出现的概率
$$

原文表示：
$$
状态集合Q=\{q_1,...,q_N\}
$$

$$
观察集合V=\{v_1,...,v_M\}
$$

$$
状态序列I=\{i_1,...,i_N\}
$$


$$
观测序列O=\{o_1,...,o_T\}
$$

$$
B=[b_j(k)],j=1,...,M\ b_j(k)表示在状态j下观察到观测k的概率
$$
模型可以表示为：
$$
\lambda=(A,B,\pi)
$$

### 前后馈计算概率

前馈：t时刻的各个状态i概率$\alpha_t(i)$乘上$A_{ij}$后求和$\sum i$得到$\alpha_{t+1}(j)$

![image-20230104004751450](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20230104004751450.png)

后馈：由状态i转化到j的概率$a^t_{ij}$乘上下一时刻j的概率$\beta_{t+1}(j)$进行求和得到此时状态i的概率

![image-20230104005338518](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20230104005338518.png)

### 一些其他符号以及期望

![image-20230104005414465](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20230104005414465.png)

![image-20230104005426935](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20230104005426935.png)

### 有监督学习参数

最大似然即可，不必多说

### 无监督学习参数-EM算法

![image-20230113005806151](https://wyj-bck.oss-cn-guangzhou.aliyuncs.com/pic/image-20230113005806151.png)

